\documentclass{lecture}

%\setbeameroption{show notes on second screen}
\usepackage{listliketab}
\usepackage{url}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{LI}
\usepackage{etex}
\usepackage{booktabs}
\usepackage{color}
\usepackage{latexsym}
\usepackage{tipa}
%\usepackage{wasysym}
\usepackage{tipa}
\usepackage{tikz}
\usepackage{tikz-qtree}
%\usepackage{textpos}
%\usepackage{MnSymbol}
\usetikzlibrary{arrows,automata,calc,positioning,through}
%\usepackage{verbatim} % for comment blocks
%\usepackage{ulem} % for strikethrough (sout)
\usepackage{pifont}
%\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{pylistings}
\usepackage{amsmath}
\usepackage{multirow}


\usefonttheme{professionalfonts} % using non standard fonts for beamer
\usefonttheme{serif} % default family is serif
%\setmainfont{fourier}

\definecolor{darkgreen}{cmyk}{0.74,0.06,0.98,0.2}

\newcommand{\red}[1]{\textcolor{usydred}{#1}}
\newcommand{\green}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\blue}[1]{\textcolor{usydblue}{#1}}
\newcommand{\gold}[1]{\textcolor{usydyellow}{#1}}

%\usepackage{beamerthemesplit}

%\usepackage{javalistings}
%\usepackage{cpplistings}
\beamertemplatenavigationsymbolsempty
%\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{bibliography item}[text]


\usepackage[style=authoryear]{biblatex}
\renewcommand*\bibopenparen{[}
\renewcommand*\bibcloseparen{]}
\addbibresource{slides.bib}

\title{A Practical Algorithm for Topic Modling with Provable Guarantees}
\author[Vanush \& Kristy]{Sanjeev Arora\\
		Rong Ge\\
		Yoni Halpern\\
		David Mimno\\
		Ankur Moitra\\
		David Sontag\\
		Yihcen Wu\\
		Michael Zhu}
\institute[\textschwa-lab]{Presented by: Vanush Vaswani and Kristy Hughes}
\date{}

\begin{document}

% Optional if you want to show outlines to structure the talk.
\AtBeginSection[]
{
  \begin{frame}
    %\frametitle{Outline: \thesection}
    \tableofcontents[currentsection]
  \end{frame}
}

\titleslide

%-------------------------------------------------%
\section[Intro]{Introduction}

\begin{plain}{Information Overload}
\vspace{-2ex}
\begin{center}
\includegraphics[scale=0.6]{figs/messy}
\end{center}
\end{plain}

\begin{plain}{Effective Organisation}
\vspace{-4ex}
\begin{center}
\includegraphics[scale=0.65]{figs/organised}
\end{center}
\end{plain}

\begin{plain}{Topics}
\vspace{-4ex}
\begin{center}
\includegraphics[scale=0.3]{figs/topic_visual}
\end{center}
\end{plain}

%-------------------------------------------------%
\section[Topic Models]{Topic Modelling}

\begin{plain}{Topics}
\begin{columns}
\column{0.3\textwidth}
\begin{center}
\includegraphics[scale=0.4]{figs/blei_topics}
\end{center}
\column{0.7\textwidth}
\begin{center}
Topics are distributions over words
\end{center}
\end{columns}
\end{plain}

\begin{plain}{Documents}
\begin{columns}
\column{0.3\textwidth}
\begin{center}
Documents have distribution of topics
\end{center}
\column{0.7\textwidth}
\begin{center}
\includegraphics[scale=0.3]{figs/blei_docs}
\end{center}
\end{columns}
\end{plain}

\begin{plain}{Topic Modelling}
\includegraphics[scale=0.35]{figs/blei}
\end{plain}

\begin{plain}{Task}
\begin{itemize}
	\p Assume documents are generated by probabilistic model with unknown variables\\
	\p Infer hidden structure onto document\\
	\p Situate new document into model\\
\end{itemize}
TODO: Redo pic
\includegraphics[scale=0.4]{figs/steps}
\end{plain}

\begin{plain}{Word-topic Matrix}
\begin{center}
\vspace{-2ex}
Extracted: Word-topic matrix
\includegraphics[scale=0.36]{figs/word_topic}\\
Aim: Find document-topic matrix
\end{center}
\end{plain}

\begin{plain}{Anchor Words}
\begin{itemize}
	\p Word-topic distributions are separable
	\p There is a word unique to each topic
	\p Indicates document is partially about that topic
	\p Can learn parameters in polynomial time provided there is a large enough number of documents
\end{itemize}
\end{plain}

\begin{plain}{Approximate Inference \& Provable Guarantees}
\begin{columns}
\column{0.55\textwidth}
\begin{itemize}
	\p Document-topic inference:
	\begin{itemize}
		\item NP-hard
	\end{itemize}	
	\p Approximate techniques
	\p Provably polynomial-time?
\end{itemize}
\column{0.55\textwidth}
\includegraphics[scale=0.4]{figs/np_approx}
\end{columns}
\end{plain}


%-------------------------------------------------%
\section[Algo]{Algorithm}

\begin{plain}{Algorithm}
Input: Corpus $\mathcal{D}$, Number of topics $K$\\
\vspace{1ex}
Output: Word-topic matrix $A$, topic-topic matrix $R$\\
\begin{enumerate}
	\item Compute word-word co-occurrence matrix
	\item Normalize the matrix
	\item Find anchor words
	\item Recover topics
\end{enumerate}
\vspace{1ex}
Assumptions:
	\begin{itemize}
		\p Topics may be correlated
		\p Word-topic distributions are separable
	\end{itemize}
\end{plain}


\begin{plain}{Contributions}
\begin{enumerate}
	\p Anchor Selection
	\begin{itemize}
		\p Combinatorial rather than ILP
		\p Stable in the presence of noise
		\p polynomial sample complexity
	\end{itemize}
	
	\p Recovery step
	\begin{itemize}
		\p Previous matrix-inversion approach sensitive to noise
		\p Replaced with Gradient-based inference
	\end{itemize}
	
	\p Empirical comparison of algorithms
\end{enumerate}
\end{plain}

%-------------------------------------------------%
\section[Anchor Selection]{Efficiently Finding Anchor Words}
\begin{plain}{Words as vertices}
TODO: Explain model of words as vertices. Lead into anchor words being words on the convex hull
\end{plain}

\begin{plain}{Convex Hull}
TODO: Explain how convex hull is computed. give time complexity for higher dimensions. Explain how convex hull "encloses" all the words within. Link aspects of problem to visual aspects
\end{plain}

\begin{plain}{Previous method}
TODO: ILP approach
\end{plain}

\begin{plain}{New method}
TODO: Iterative approach
\end{plain}

%-------------------------------------------------%
\section[Topic Recovery]{Topic Recovery via Bayes' Rule}
\begin{plain}{Problem}
TODO: What is topic recovery
\end{plain}

\begin{plain}{Previous method}
TODO: Overview of approach
\end{plain}

\begin{plain}{Matrix Inversion}
TODO: How matrix inversion works
TODO: How this recovered topics
\end{plain}

\begin{plain}{New method}
TODO: Overview of approach
\end{plain}

\begin{plain}{Bayes' Rule}
TODO: Bayes rule and how it relates to new method
\end{plain}

%-------------------------------------------------%
\section[Results]{Experimental Results}
\begin{plain}{Experiments}
TODO: overview of the experiments run
\end{plain}

\begin{plain}{Metrics}
TODO: Metrics
\end{plain}

\begin{plain}{Documents}
TODO: Talk about semi-synthetic documents, real documents and the need for both
\end{plain}

\begin{plain}{Results}
TODO: describe results. Iterate through each experiment, and each document type, reporting the computed metrics for each. This may need to be split up into more slides by either experiment or document type
\end{plain}

%-------------------------------------------------%
\section[Conclusion]{Conclusion}
\begin{plain}{Summary}
TODO: Put the paper's conclusion into dot point form
\end{plain}

\begin{plain}{Comments}
TODO: Do we need to comment on the paper?\\
Are there things that we wish they had reported but didn't?\\
Are there things that we really liked that they reported?\\
Check the marking guidelines about what exactly we need here\\
\end{plain}

\begin{plain}{Future Work}
TODO: They didn't have a future work section but they really should have. We can make one up and maybe comment that they didn't put a future work section
\end{plain}

\begin{plain}{}
\begin{center}
\Huge
Thanks!\\
\vspace{3ex}
\Large
Any questions please email either of us:\\
\vspace{2ex}
\large
\textbf{Vanush Vaswani}\\
\normalsize
vvas****@uni.sydney.edu.au\\
\vspace{1ex}
\large
\textbf{Kristy Hughes}\\
\normalsize
khug2372@uni.sydney.edu.au
\end{center}
\end{plain}

\end{document}
