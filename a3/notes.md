Notes for the presentation
==========================

Introduction
------------

- Information Overload -
With hundreds of terabytes of data existing on the internet it is becoming harder and harder to access the data in a useful and orderly way. This introduces the need for new tools and algorithms to organise and search more effectively.
An intuitive way to organise documents is to group them by topic. However, since topics are abstract and one document can have many topics, this is a difficult task for a computer to automatically perform.

- Topics - 
Topic modeling aims to uncover hidden thematic structure within documents. Topics, in this case, are defined as a probability distribution of terms over the vocabulary of a corpus. Here, we have four topics, and words with a high probability are written in a large font and words with a low probability are written in a small font.


Topic Modelling
----------------

- Topics -
Each topic is modelled as a distribution over words.

- Documents -
Each document is modelled as a distribution over topics

- Topic Modelling -
The generative process for a document begins by drawing the documents topic distribution. Then, for each position, we sample a topic assignment and a word.The idea of the problem is that we assume that each document was generated through this process. So the document is essentially a bag of words, where each word was produced at random by a topic. A document where the word "gene" appears a lot of times has a higher probability that it is about the yellow topic because that is the topic that is most likely to have generated that word. The problem of topic modeling is this: Given a set of topics (that is a set of probability distributions over words), can we work out which topics generated a given document? To do this, we begin by combining the column vectors of each topics word distribution into a word-topic matrix. 

- Task -
The algorithm usually proceeds as the following:
1) Treat data as observations resulting from a probabilistic process with hidden variables
2) Infer hidden structure with posterior inference
3) Situate new data into estimated model
There is a bit of associated jargon with topic modelling, namely the word-topic matrix and anchor words. We will describe them a little now.

- Word-topic Matrix -
The word-topic matrix gives us an idea of which words best describe each topic and which words are common overall. For example, the word 'online' appears in multiple topics, so would not be very useful for use in a specific description of a single topic. The aim of the task is to find a distribution of topics for each document.

- Anchor Words -
In order to understand this algorithm fully, we must first go into the definition of anchor words. Th authors 2012 paper present an algorithm that provably learns the parameters of a topic model provided that the word-topic distributions are separable. A word-topic matrix is p-separable if for each topic, there is a word where its probability of being generated by that topic is greater than p, and the probability of it being generated by all other topics is 0. In other words, topics are separable if there exists a word that is unique to each topic. This word is called an anchor word. If a document contains an anchor word, then we know that it must be at least partially about the corresponding topic, since no other topic is able to generate that word (the probability of an anchor word being generated by any other topic is by definition 0). So assuming that this assumption holds true, the authors presented an algorithm in their previous paper that can learn the parameters of a topic model in polynomial time provided that the number of documents is large enough. This document lower-limit is actually well defined, and the equation for it can be found in the paper. It increases with the size of vocab, and number of topics.

- Approximate Inference & Provable Guarantees-
The approach to topic modelling assumes that each document was generated using the hypothesised model and the task is to statistically recover the parameters of the model. Current research in topic modelling uses approximate inference techniques such as Singular Value Decomposition, variational inference and Markov Chain Monte Carlo. This is because posterior inference of document-topic (and topic-word) distributions is very difficult. In the worst case, it is NP-hard. Many current approaches are heuristic: we cannot prove good bounds on either their performance or their running time. So work is needed to design provably polynomial-time algorithms. 

Algorithm
---------

- Overview -
This paper is based off the algorithm the authors wrote in 2012. The algorithm takes the second order moment matrix of word-word co-occurrences as the input. It then finds anchor words for each topic and uses them to reconstruct topic distributions.
This paper changes how the anchor words are found and the topics are recovered in order to make it run in polynomial time.

- Contributions -
The contributions this paper makes to the field are incremental - it changes the mechanisms used in an existing algorithm to increase speed and stability. It inherits the provable guarantees of their previous paper but uses simple and practical implementation. In summary, this paper moves away from solving integer linear programs in the anchor selection phase, using a combinatorial approach instead. It also uses gradient-based inference as opposed to using matrix inversion for computing topic-word distributions.

