Notes for the presentation
==========================

Introduction
------------

- Information Overload -
With hundreds of terabytes of data existing on the internet it is becoming harder and harder to access the data in a useful and orderly way. This introduces the need for new tools and algorithms to organise and search more effectively.
An intuitive way to organise documents is to group them by topic. However, since topics are abstract and one document can have many topics, this is a difficult task for a computer to automatically perform.

- Topics - 
Topic modeling aims to uncover hidden thematic structure within documents. Topics, in this case, are defined as a probability distribution of terms over the vocabulary of a corpus. Here, we have four topics, and words with a high probability are written in a large font and words with a low probability are written in a small font.


Topic Modelling
----------------

- Topics -
Each topic is modelled as a distribution over words.

- Documents -
Each document is modelled as a distribution over topics

- Topic Modelling -
The generative process for a document begins by drawing the documents topic distribution. Then, for each position, we sample a topic assignment and a word.The idea of the problem is that we assume that each document was generated through this process. So the document is essentially a bag of words, where each word was produced at random by a topic. A document where the word "gene" appears a lot of times has a higher probability that it is about the yellow topic because that is the topic that is most likely to have generated that word. The problem of topic modeling is this: Given a set of topics (that is a set of probability distributions over words), can we work out which topics generated a given document? To do this, we begin by combining the column vectors of each topics word distribution into a word-topic matrix. 

- Task -
The algorithm usually proceeds as the following:
1) Treat data as observations resulting from a probabilistic process with hidden variables
2) Infer hidden structure with posterior inference
3) Situate new data into estimated model
There is a bit of associated jargon with topic modelling, namely the word-topic matrix and anchor words. We will describe them a little now.

- Word-topic Matrix -
The word-topic matrix gives us an idea of which words best describe each topic and which words are common overall. For example, the word 'online' appears in multiple topics, so would not be very useful for use in a specific description of a single topic. The aim of the task is to find a distribution of topics for each document.

- Anchor Words -
In order to understand this algorithm fully, we must first go into the definition of anchor words. Th authors 2012 paper present an algorithm that provably learns the parameters of a topic model provided that the word-topic distributions are separable. A word-topic matrix is p-separable if for each topic, there is a word where its probability of being generated by that topic is greater than p, and the probability of it being generated by all other topics is 0. In other words, topics are separable if there exists a word that is unique to each topic. This word is called an anchor word. If a document contains an anchor word, then we know that it must be at least partially about the corresponding topic, since no other topic is able to generate that word (the probability of an anchor word being generated by any other topic is by definition 0). So assuming that this assumption holds true, the authors presented an algorithm in their previous paper that can learn the parameters of a topic model in polynomial time provided that the number of documents is large enough. This document lower-limit is actually well defined, and the equation for it can be found in the paper. It increases with the size of vocab, and number of topics.

- Approximate Inference & Provable Guarantees-
The approach to topic modelling assumes that each document was generated using the hypothesised model and the task is to statistically recover the parameters of the model. Current research in topic modelling uses approximate inference techniques such as Singular Value Decomposition, variational inference and Markov Chain Monte Carlo. This is because posterior inference of document-topic (and topic-word) distributions is very difficult. In the worst case, it is NP-hard. Many current approaches are heuristic: we cannot prove good bounds on either their performance or their running time. So work is needed to design provably polynomial-time algorithms. 

Algorithm
---------

- Algorithm -
This paper is based off the algorithm the authors wrote in 2012. The algorithm takes the second order moment matrix of word-word co-occurrences as the input. It then finds anchor words for each topic and uses them to reconstruct topic distributions.
This paper changes how the anchor words are found and the topics are recovered in order to make it run in polynomial time.

- Contributions -
The contributions this paper makes to the field are incremental - it changes the mechanisms used in an existing algorithm to increase speed and stability. It inherits the provable guarantees of their previous paper but uses simple and practical implementation. In summary, this paper moves away from solving integer linear programs in the anchor selection phase, using a combinatorial approach instead. It also uses gradient-based inference as opposed to using matrix inversion for computing topic-word distributions.

Anchor Selection
----------------
- Word-word co-occurrence matrix -
Here we consider the normalised word-word co-occurrence matrix of the document set where each row and column represents a word and the number in each cell represents how many times those pairs of words appear in the same document.

- Words as vertices-
If we plot each row in V dimensional space, we get a collection of words mapped to points, with the position of the point representing how similar they are to other words. It is a little hard to visualise dimensions higher than 3, but essentially words that relate to similar topics will end up being clustered together, while words that are very dissimilar are also far distance-wise. So using this image, we can see that points on the outside are going to be very significant.
TODO: Do you prefer this picture or vertex.jpg

- Convex Hull -
The set of points that enclose all other points in a convex polyhedron is called the convex hull. On the left we have a 2d convex hull, and on the right we have a 3d convex hull. The convex hull of the plotted words we just described has the same number of dimensions as words. It turns out that the points on the outside are the anchor words. So the method that the authors have presented is to compute the convex hull of the set of points. The points that make up the convex hull are the anchor words.
TODO: Add stuff about a simplex here

- Computing Convex Hull -
Computing the convex hull of a 2 dimensional point set is efficient, but it is not for a greater number of dimensions. The exact computational complexity depends on the method used. The authors previous paper solved an integer linear program for each point to determine whether or not it is part of the hull. This is not necessarily bounded, let alone polynomial in time. The method proposed in this paper uses a recursive algorithm which is basically a greedy algorithm maximizing the volume of the polytope formed by the points.
For all iterative algorithms, we need a method of choosing the first point. This is done by randomly projecting the points into a lower dimensional subspace and then choosing the point furthest from the origin. 
The algorithm iteratively adds a point to the convex hull. Given a set of points that are part of the convex hull, the algorithm computes the subspace span of the points, and then adds the point furthest from this subspace to the convex hull. Each point in the convex hull corresponds to an anchor word, which is like the ID of a topic (since by definition it has 0 probability in all other topics). So the algorithm stops when the specified number of topics (K) is found.
This approach can also be seen as a greedy approach to maximize the volume

Topic Recovery
--------------

- Topic Recovery Task -
The topic recovery step aims to extract the distributions over words for each topic. Since we have found our K anchor words, they act as a unique identifier for each topic. But since topics are defined as distributions of words, we need to find the topic-word distributions.

- Previous Method -
In the authors previous paper, they performed topic recovery using matrix inversion. They permuted the word-word co-occurrence matrix, Q,  so that the first K rows and columns contain the anchor words and then discard the remaining rows and columns. This produces a diagonal matrix because by definition an anchor word has 0 probability for all other words. They then formulated the relationship between the this matrix, Q, and word topic matrix A and the TODO (What does R represent) R. This equation was solved using matrix inversion

- Word-word co-occurrence probability matrix -
The new method uses a variation on the word-word co-occurrence matrix, Q. It normalises it into the probability of each word co-occurring. Here, the variable w represents a word and the variable z represents the topic. For an anchor word, this first probability is 1 because the probability is 0 for all other words by definition. So this simplifies to the probability of a word given a topic. But for any other row, this probability isnt 1 so it remains as this equation. Since this is a convex combination of the anchor words, we can simplify it down to this sum instead.

- New Method -
So now that we have gone through some of the equations we are able to understand the algorithm presented. The authors first normalise the word-word co-occurrence matrix as we just describe. They then compute A and R using Bayes rule
TODO: expand on this


Results
-------

- Methodology overview -
The algorithms that were tested are the original recovery method, the topic recovery method presented in this paper, and a common MCMC based method based on Gibbs sampling of the posterior distribution. The experiments cover various aspects of the algorithms. They generate completely synthetic documents to test the efficiency of each of the recovery algorithms presented. Using two-real world datasets, they generate semi-synthetic corpora using an LDA model to compute the reconstruction error from the true word-topic distributions and the trained model.  For real documents, they use metrics such as held-out probability and coherence to quantify the performance of the model where the true parameters are unavailable.

Two real world datasets were used throughout - a large corpus of NYT article (295,000 documents) and NIPS abstracts (1100 documents)

- Metrics -
The reconstruction error is calculated by comparing the learned word-topic matrix and true matrix A and then evaluating the L1 distance between each pair of topics. Where the true parameters are unknown, they use the held-out probability, by calculating the probability of an unseen document under the learned model. If the model is good, then the probability will be high. If the model is good, then high probability words for a certain topic should co-occur frequently. This is measured by the coherence metric, which has been shown to correlate well with human judgement. Inter-topic similarity gathers the set of the N most probable words and counts how many of those words do not appear in any other topic’s set of N most probable words, thereby measuring the discerning power of a topic under the model. We can summarize this as the number of unique words across topics.

- Results -
The efficiency results show that the Gibbs sampling (MCMC) and RecoverL2 method is linear in corpus size, however, the RecoverKL and Recover algorithms have performance invariant of corpus size. The FastAnchorWords algorithm, described previously, takes less than 6 seconds for all corpora.

- Results -
The reconstruction error of the word-topic matrix for semi-synthetic corpora is now presented. It shows that the L1 error drops significantly as the number of documents in increased, as does the variance represented by the error bars. Also included is the so-called infinite data case, where the ground truth word-topic matrix, having the lowest error. This is significant as it shows that their model of the Q matrix as an approximation to expectation is a good one.

- Results -
This chart shows the metrics for real documents, that is, when the true parameters are unknown. The held-out probability is worst for the Recover algorithm, while Gibbs sampling produces the best. However, it has worse performance in coherence and better performance in the number of unique words per topic. 


Conclusion
----------

- Conclusion -
It is clear from the previous slides that the empirical results compare to common approaches in the literature such as MCMC by Gibbs sampling. These new algorithms for topic recovery use Bayes’ rule. By using Bayes rule to compute the word-topic matrix allows an order-of-magnitude improvement in performance, allowing running time to be independent of the size of the corpus. 

- Future work -
Future work involves using the output of the algorithms as a prior step to further optimization (e.g. combining with MCMC), though they have not found a hybrid that outperforms either method independently. Future parallel implementations have the potential to support web-scale topic inference

- Comments -
This paper provides a provably polynomial time algorithm which is an improvement on current algorithms which rely on purely empirical methods. 
It transforms a difficult statistical problem into a simpler geometric one and solves it with Bayes rule. 
However, the contribution remains an incremental as it does not change the field of topic modelling but rather improves the performance

- Thanks _
Well thats it. Thanks for listening to our talk. Usually we would ask for questions but since this is a video talk we cant do that. So instead, if you have any questions please feel free to email either of us at our emails we have here.
Cya!
